{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMkLIsZGY15QOoAtgqb4TE5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcSh8uEjlLnT","executionInfo":{"status":"ok","timestamp":1674043179665,"user_tz":-330,"elapsed":6980,"user":{"displayName":"CE138_Meet_Sukhadiya","userId":"17999769234007530522"}},"outputId":"29e260b4-6242-4103-cd4c-1e80c815ae84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["4102      @klariza that's awesome! love stuff look forwa...\n","818850    hi @juliaarielle ! would suggest music electro...\n","806099    cute... i'm not maudlin mood despite may appea...\n","3535      wow, way early awake. lots though, software po...\n","812867    fun night! time bed... tomorrow going test. ni...\n","                                ...                        \n","809845                 @esb0727 well, i'm glad it's not me.\n","806414    ... wish real time machine though.... shaped l...\n","819252                             best chicken cutlet ever\n","814311                          @thecolorabi waddup friend.\n","13942     first swim year arms empty 35 mins drills free...\n","Name: text, Length: 10000, dtype: object"]},"metadata":{},"execution_count":31}],"source":["import pandas as pd\n","from nltk.corpus import stopwords\n","import string\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import classification_report\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user_id\", \"text\"]\n","df = pd.read_csv(\"/content/drive/MyDrive/archive/training.1600000.processed.noemoticon.csv\", encoding=\"latin\",names=columns)\n","\n","df[\"sentiment\"] = df[\"sentiment\"].replace(4,1)\n","df['sentiment'].unique()\n","data_pos = df[df['sentiment'] == 1]\n","data_neg = df[df['sentiment'] == 0]\n","data_pos = data_pos.iloc[:int(20000)]\n","data_neg = data_neg.iloc[:int(20000)]\n","dataset = pd.concat([data_pos, data_neg]) \n","dataset['text']=dataset['text'].str.lower()\n","dataset['text'].tail()\n","\n","stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n","             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n","             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n","             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n","             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n","             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n","             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n","             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n","             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n","             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n","             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n","             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n","             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n","             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n","             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n","STOPWORDS = set(stopwordlist)\n","def cleaning_stopwords(text):\n","    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n","dataset['text'] = dataset['text'].apply(lambda text: cleaning_stopwords(text))\n","dataset['text'].sample(10000)"]}]}